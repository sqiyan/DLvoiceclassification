{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrqRK8C8Sz-m"
      },
      "source": [
        "# Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWSgv5m76kMD",
        "outputId": "3a49e879-8e0e-47c9-afb8-2208fbdb8992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN_v1.ckpt  drive  file.wav  recording.webm  requirements.txt\tsample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "-Z2URWqX8Wd6"
      },
      "outputs": [],
      "source": [
        "# file path for our trained model\n",
        "model_path = \"CNN_v1.ckpt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tigPi8LPeMsX"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp_izuI-cSp_",
        "outputId": "9a1ee3e8-95b3-4a60-dd78-08dac27c16ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.8.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.64.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.8.0)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2022.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.8)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.44.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "34rUKUc8eQ-M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch.utils.data as data\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa5-A39G1ehk"
      },
      "source": [
        "# Audio preprocessing & mel spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "DoFuJgyMP_X4"
      },
      "outputs": [],
      "source": [
        "#Output: tensor\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio import transforms\n",
        "\n",
        "class AudioUtil():\n",
        "    @staticmethod\n",
        "    def open(audio_file):\n",
        "        #load audio file\n",
        "        signal, sample_rate = torchaudio.load(audio_file)\n",
        "        return signal, sample_rate\n",
        "\n",
        "    @staticmethod\n",
        "    def standardize_channel(aud):\n",
        "        #to standardize the audio files to 1 channel (in case some have 2)\n",
        "        signal, sample_rate = aud\n",
        "\n",
        "        if signal.shape[0] > 1:\n",
        "          signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "\n",
        "        return signal, sample_rate\n",
        "\n",
        "    @staticmethod\n",
        "    def resampling(aud, target_sr):\n",
        "      signal, sample_rate = aud\n",
        "\n",
        "      if sample_rate == target_sr:\n",
        "        return aud\n",
        "\n",
        "      else:\n",
        "        channel = signal.shape[0]\n",
        "        resampled = torchaudio.transforms.Resample(sample_rate, target_sr)(signal[:1,:])\n",
        "\n",
        "        return resampled, target_sr\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def standardize_duration(aud, max_time):\n",
        "        #standardize all audio files to the same length by either extending duration with silence or truncating it\n",
        "        signal, sample_rate = aud\n",
        "        num_of_rows, signal_length = signal.shape\n",
        "        max_length = sample_rate//1000 * max_time\n",
        "\n",
        "        if (signal_length > max_length):\n",
        "            #truncate signal to given length\n",
        "            signal = signal[:, :max_length]\n",
        "\n",
        "        elif (signal_length < max_length):\n",
        "            #length of padding to add\n",
        "            padding_len = max_length - signal_length\n",
        "\n",
        "            #pad with 0s\n",
        "            padding = torch.zeros(num_of_rows, padding_len)\n",
        "\n",
        "            signal = torch.cat((signal, padding), 1)\n",
        "\n",
        "        return signal, sample_rate\n",
        "\n",
        "    @staticmethod\n",
        "    def time_shift(aud, shift_limit):\n",
        "        #data augmentation on raw audio by time shifting to left/right by a random amount\n",
        "        signal, sample_rate = aud\n",
        "        _, signal_length = signal.shape\n",
        "        amount_to_shift = int(random.random() * shift_limit * signal_length)\n",
        "\n",
        "        return signal.roll(amount_to_shift), sample_rate\n",
        "\n",
        "    @staticmethod\n",
        "    def spectro_gram(aud, n_mels=64, n_fft = 1024, hop_len=None):\n",
        "        #convert augmented audio to a mel spectrogram\n",
        "        signal, sample_rate = aud\n",
        "        top_db = 100\n",
        "\n",
        "        spec = transforms.MelSpectrogram(sample_rate, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(signal)\n",
        "\n",
        "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
        "        return spec\n",
        "\n",
        "    @staticmethod\n",
        "    def mel_spectrogram_augment(spec, max_mask=0.1, n_freq_masks=1, n_time_masks=1):\n",
        "        #another round of augmentation, on mel spectrogram rather than raw data\n",
        "        #frequency mask and time mask\n",
        "        _, n_mels, n_steps = spec.shape\n",
        "        mask_value = spec.mean()\n",
        "        aug_spec = spec\n",
        "\n",
        "        #freq_mask_param: max possible time of the mask\n",
        "        freq_mask_param = max_mask * n_mels\n",
        "        for _ in range (n_freq_masks):\n",
        "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
        "\n",
        "        #time_mask_param: max possible time of the mask\n",
        "        time_mask_param = max_mask * n_steps\n",
        "        for _ in range (n_time_masks):\n",
        "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
        "\n",
        "        return aug_spec\n",
        "\n",
        "def example_spec_from_aud(audio_path):\n",
        "    # Sample preprocessing pipeline\n",
        "    aud = AudioUtil.open(audio_path)\n",
        "    #reaud = AudioUtil.resampling(aud, 8000) #change the number to the sampling rate you want, here is 8khz\n",
        "    mono = AudioUtil.standardize_channel(aud)\n",
        "    fixed_duration = AudioUtil.standardize_duration(mono, 5000) #duration = 5s\n",
        "    shift_aud = AudioUtil.time_shift(fixed_duration, 0.4) #40%\n",
        "    spectrogram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "    #n_mels = 64 because that is the normal speaking vocal range\n",
        "    aug_sgram = AudioUtil.mel_spectrogram_augment(spectrogram, max_mask=0.1, n_freq_masks=2, n_time_masks=2)\n",
        "    return aug_sgram\n",
        "\n",
        "\n",
        "#######################################\n",
        "###     preprocessing functions     ###\n",
        "#######################################\n",
        "\n",
        "def raw_from_aud(aud):\n",
        "    # Takes in raw aud, outputs preprocessed raw signal\n",
        "    # Edit this to tune preprocessing\n",
        "    \n",
        "    new_aud = AudioUtil.resampling(aud, 8000) #change the number to the sampling rate you want, here is 8khz\n",
        "    new_aud = AudioUtil.standardize_channel(new_aud)\n",
        "    signal, sr = AudioUtil.standardize_duration(new_aud, 5000) #duration = 5s\n",
        "\n",
        "    return signal\n",
        "\n",
        "def spec_from_aud(aud):\n",
        "    # Takes in raw aud, outputs preprocessed mel spectrogram\n",
        "    # Edit this to tune preprocessing\n",
        "\n",
        "    new_aud = AudioUtil.standardize_channel(aud)\n",
        "    new_aud = AudioUtil.standardize_duration(new_aud, 5460) #duration = 5.46s to get mel output length of 512\n",
        "    new_aud = AudioUtil.time_shift(new_aud, 0.4) #40%\n",
        "    spectrogram = AudioUtil.spectro_gram(new_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "    #n_mels = 64 because that is the normal speaking vocal range\n",
        "    spectrogram = AudioUtil.mel_spectrogram_augment(spectrogram, max_mask=0.1, n_freq_masks=2, n_time_masks=2)\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "def print_spectrogram(tensor_input):\n",
        "  #time vs amplitude\n",
        "  two_d_spec = tensor_input[0]\n",
        "  return plt.imshow(two_d_spec.permute(0,1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84zXhG71Djw-"
      },
      "source": [
        "# Viewing dataset and Obtaining Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "tUS8sLrcawQt"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "###  Parameters to change  ###\n",
        "##############################\n",
        "\n",
        "# Put your preprocessing functions here\n",
        "PREPROCESSOR = spec_from_aud\n",
        "\n",
        "BATCH_SIZE = 1  # Default 64\n",
        "# If we were to have variable input length, keep batch_size at 1 or implement collate_fn()\n",
        "\n",
        "# This cuts the dataset down to have faster iterations during prototyping\n",
        "REDUCED_DATASET = True\n",
        "\n",
        "num_workers = 4\n",
        "# Need to experiment in this parameter.\n",
        "# Colab gives 2 CPU cores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlFu63MhVlfW"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLh3AQVFxoV5",
        "outputId": "71dce174-b46b-4db3-a256-8aaaa1a1b290"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3bf35032f0>"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define training parameters\n",
        "\n",
        "loss_function = F.nll_loss\n",
        "\n",
        "optimizer = torch.optim.Adam\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 50\n",
        "torch.manual_seed(28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1YWxvMXXmpF"
      },
      "source": [
        "We define the model here for the model file to be loaded into a PyTorch model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "8BRee32EViH6"
      },
      "outputs": [],
      "source": [
        "# CNN model implementation for Mels spectrogram\n",
        "class CustomModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=6, any_other_params_you_need=None):\n",
        "        super().__init__()\n",
        "        self.H, self.W = 64, 512  # Size of mel_spectrogram\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define your layers here:\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=\"same\")\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.maxpool1 = nn.MaxPool2d(2, stride=2) #max pooling\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 16, 5, padding=\"same\")\n",
        "        self.maxpool2 = nn.MaxPool2d(2, stride=1) #max pooling\n",
        "\n",
        "        self.conv3 = nn.Conv2d(16, 8, 2, padding=\"same\")\n",
        "        self.bn2 = nn.BatchNorm2d(8)\n",
        "        self.fc1 = nn.Linear(8 * 31 * 255, self.num_classes)\n",
        "\n",
        "        self.pooling2x2 = lambda x: F.max_pool2d(x, 2, stride=2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = F.relu(self.conv1(inputs))\n",
        "        x = self.bn1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x = x.view(inputs.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "_TF2lD6kvxSj"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "\n",
        "class LightningModel(pl.LightningModule):\n",
        "    def __init__(self, model, learning_rate=1e-3, loss_function=F.nll_loss, optimizer=torch.optim.Adam, weight_decay=1e-6):\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_function = loss_function\n",
        "        # Weight decay for L2 regularization\n",
        "        self.optimizer = optimizer(model.parameters(), lr=self.learning_rate, weight_decay=weight_decay)\n",
        "        self.model = model\n",
        "        \n",
        "        self.train_acc = torchmetrics.Accuracy()\n",
        "        self.val_acc = torchmetrics.Accuracy()\n",
        "        self.test_acc = torchmetrics.Accuracy()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # in lightning, forward defines the prediction/inference actions\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defined the train loop.\n",
        "        # It is independent of forward\n",
        "        x, y = batch\n",
        "        output = self(x)  # Call self.forward function\n",
        "        loss = self.loss_function(output, y)\n",
        "        self.train_acc(output, y)\n",
        "        # Logging to TensorBoard by default\n",
        "        self.log(\"train_loss\", loss, on_epoch=True)\n",
        "        self.log(\"train_acc\", self.train_acc, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # training_step defined the train loop.\n",
        "        # It is independent of forward\n",
        "        x, y = batch\n",
        "        output = self(x)  # Call self.forward function\n",
        "        loss = self.loss_function(output, y)\n",
        "        self.val_acc(output, y)\n",
        "        # Logging to TensorBoard by default\n",
        "        self.log(\"val_loss\", loss, on_epoch=True)\n",
        "        self.log(\"val_acc\", self.val_acc, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # training_step defined the train loop.\n",
        "        # It is independent of forward\n",
        "        x, y = batch\n",
        "        output = self(x)  # Call self.forward function\n",
        "        loss = self.loss_function(output, y)\n",
        "        self.test_acc(output, y)\n",
        "        # Logging to TensorBoard by default\n",
        "        self.log(\"test_loss\", loss, on_epoch=True)\n",
        "        self.log(\"test_acc\", self.test_acc, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return self.optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9m087-HuKiX"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXYMEBwvYEE1"
      },
      "source": [
        "Once the model has been loaded, when we run `print(model)`, we should see a summary of the model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwQKAmIIuPXB",
        "outputId": "3d09b986-c805-4256-c19a-2a0815c71b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightningModel(\n",
            "  (model): CustomModel(\n",
            "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv2): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "    (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3): Conv2d(16, 8, kernel_size=(2, 2), stride=(1, 1), padding=same)\n",
            "    (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (fc1): Linear(in_features=63240, out_features=6, bias=True)\n",
            "  )\n",
            "  (train_acc): Accuracy()\n",
            "  (val_acc): Accuracy()\n",
            "  (test_acc): Accuracy()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Initialise a new model and load the state\n",
        "model = LightningModel(CustomModel(), learning_rate=learning_rate, loss_function=loss_function, optimizer=optimizer)\n",
        "\n",
        "# code for loading checkpoint file\n",
        "checkpoint = torch.load(model_path)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bJTNqqerBLN"
      },
      "source": [
        "# Let the model guess your age!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9skn9HNplLw",
        "outputId": "4e6d9ec8-8bb3-484e-d5bf-125489e369b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: ipywebrtc in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (5.3.1)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchaudio) (4.1.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook) (0.13.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook) (5.3.0)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook) (5.1.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook) (5.6.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook) (5.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook) (2.11.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook) (0.2.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook) (1.8.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook) (4.10.1)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook) (4.9.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook) (22.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client>=5.2.0->notebook) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook) (0.7.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->notebook) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook) (2.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (5.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (1.5.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook) (2.15.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook) (5.7.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook) (4.11.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook) (3.8.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook) (0.5.1)\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n"
          ]
        }
      ],
      "source": [
        "# set up microphone audio recorder\n",
        "\n",
        "!sudo apt install ffmpeg\n",
        "!pip install torchaudio ipywebrtc notebook\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "\n",
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "import torchaudio\n",
        "from IPython.display import Audio\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "wfCWwdK5qNWv"
      },
      "outputs": [],
      "source": [
        "# initialise audio recorder\n",
        "\n",
        "camera = CameraStream(constraints={'audio': True,'video':False})\n",
        "recorder = AudioRecorder(stream=camera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcBePRwGYx6o"
      },
      "source": [
        "Click on the dot button to start recording, and click on it again to stop recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107,
          "referenced_widgets": [
            "58acd8e6f8d5409484b32ced3b49707d",
            "d7f4af699e1a491f9de6ac21bfd1c75b",
            "cfd5520656a04f3c8a2b6dc77de9274a",
            "c3ed5f5b1fce4cf28cacbfa4a7309494",
            "0353b6212c7f4e11b0090c179836d0a2",
            "ed9069eb8905422b85da9248994f5b4c"
          ]
        },
        "id": "TF8A6osuqRoB",
        "outputId": "4578e1c5-3966-4052-8bba-b662a69fa25b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58acd8e6f8d5409484b32ced3b49707d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "AudioRecorder(audio=Audio(value=b'', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': …"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/a8874ba6619b6106/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# runs audio recorder interface\n",
        "\n",
        "recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "iT7itfSLqvw9"
      },
      "outputs": [],
      "source": [
        "# convert audio recording into .wav\n",
        "\n",
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder.audio.value)\n",
        "!ffmpeg -i recording.webm -ac 1 -f wav file.wav -y -hide_banner -loglevel panic\n",
        "sig, sr = torchaudio.load(\"file.wav\") # converted audio is saved as file.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "406dTYzQrTc8",
        "outputId": "dfd740f7-3a9d-4daf-fb13-5240e41e163d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted tensor([0])\n"
          ]
        }
      ],
      "source": [
        "# pass in file.wav into model for prediction\n",
        "\n",
        "pred = model(torch.unsqueeze(PREPROCESSOR(torchaudio.load(\"file.wav\")),0))\n",
        "pred_label = torch.argmax(pred, dim=1)\n",
        "\n",
        "print(\"predicted\", pred_label)\n",
        "\n",
        "label_to_classification = {0: \"TEENS\", 1: \"TWENTIES\", 2: \"THIRTIES\", \n",
        "                           3: \"FOURTIES\", 4: \"FIFTIES\", 5: \"SIXTIES\"}\n",
        "classification = label_to_classification[pred_label.item()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y19MBiA7rAK9"
      },
      "source": [
        "# The model thinks you are.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2zj6tAdZO-6",
        "outputId": "2120f4cc-e9ef-4410-b250-493656b14779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From your voice... \n",
            "\n",
            "The model thinks you are in your TEENS!\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"From your voice... \n",
        "\n",
        "The model thinks you are in your {classification}!\n",
        "  \"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Project Code for GUI",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0353b6212c7f4e11b0090c179836d0a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58acd8e6f8d5409484b32ced3b49707d": {
          "model_module": "jupyter-webrtc",
          "model_module_version": "~0.6.0",
          "model_name": "AudioRecorderModel",
          "state": {
            "_data_src": "blob:https://lioo294yau8-496ff2e9c6d22116-0-colab.googleusercontent.com/d9c456f1-ce34-4318-8ec9-71d9f0630203",
            "_dom_classes": [],
            "_model_module": "jupyter-webrtc",
            "_model_module_version": "~0.6.0",
            "_model_name": "AudioRecorderModel",
            "_view_count": null,
            "_view_module": "jupyter-webrtc",
            "_view_module_version": "~0.6.0",
            "_view_name": "AudioRecorderView",
            "audio": "IPY_MODEL_d7f4af699e1a491f9de6ac21bfd1c75b",
            "autosave": false,
            "codecs": "",
            "filename": "record",
            "format": "webm",
            "layout": "IPY_MODEL_cfd5520656a04f3c8a2b6dc77de9274a",
            "recording": false,
            "stream": "IPY_MODEL_c3ed5f5b1fce4cf28cacbfa4a7309494"
          }
        },
        "c3ed5f5b1fce4cf28cacbfa4a7309494": {
          "model_module": "jupyter-webrtc",
          "model_module_version": "~0.6.0",
          "model_name": "CameraStreamModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "jupyter-webrtc",
            "_model_module_version": "~0.6.0",
            "_model_name": "CameraStreamModel",
            "_view_count": null,
            "_view_module": "jupyter-webrtc",
            "_view_module_version": "~0.6.0",
            "_view_name": "MediaStreamView",
            "constraints": {
              "audio": true,
              "video": false
            },
            "layout": "IPY_MODEL_ed9069eb8905422b85da9248994f5b4c"
          }
        },
        "cfd5520656a04f3c8a2b6dc77de9274a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f4af699e1a491f9de6ac21bfd1c75b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "AudioModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "AudioModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "AudioView",
            "autoplay": true,
            "controls": true,
            "format": "webm",
            "layout": "IPY_MODEL_0353b6212c7f4e11b0090c179836d0a2",
            "loop": true
          }
        },
        "ed9069eb8905422b85da9248994f5b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
