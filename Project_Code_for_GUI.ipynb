{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Welcome to our voice classification by age model**\n",
        "Let our trained CNN model guess your age from your voice! \n",
        "\n",
        "To begin, select **Run All Cells** to set up the environment, functions, and load our PyTorch model\n",
        "\n",
        "Next, head to the **'Record your audio here!' section** of the notebook to record your audio using the AudioRecorder interface\n",
        "\n",
        "Then, continue running the rest of the cells to receive your age prediction :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrqRK8C8Sz-m"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install dependencies \n",
        "!pip install -r packages.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWSgv5m76kMD",
        "outputId": "3a49e879-8e0e-47c9-afb8-2208fbdb8992"
      },
      "outputs": [],
      "source": [
        "# check if model file is in directory\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z2URWqX8Wd6"
      },
      "outputs": [],
      "source": [
        "# file path for our trained model\n",
        "model_path = \"CNN_v1.ckpt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tigPi8LPeMsX"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34rUKUc8eQ-M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch.utils.data as data\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa5-A39G1ehk"
      },
      "source": [
        "## Audio preprocessing & mel spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoFuJgyMP_X4"
      },
      "outputs": [],
      "source": [
        "class AudioUtil():\n",
        "    @staticmethod\n",
        "    def open(audio_file):\n",
        "        #load audio file\n",
        "        signal, sample_rate = torchaudio.load(audio_file)\n",
        "        return signal, sample_rate\n",
        "\n",
        "    @staticmethod\n",
        "    def standardize_channel(aud):\n",
        "        #to standardize the audio files to 1 channel (in case some have 2)\n",
        "        signal, sample_rate = aud\n",
        "\n",
        "        if signal.shape[0] > 1:\n",
        "          signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "\n",
        "        return signal, sample_rate\n",
        "\n",
        "    @staticmethod\n",
        "    def resampling(aud, target_sr):\n",
        "      signal, sample_rate = aud\n",
        "\n",
        "      if sample_rate == target_sr:\n",
        "        return aud\n",
        "\n",
        "      else:\n",
        "        channel = signal.shape[0]\n",
        "        resampled = torchaudio.transforms.Resample(sample_rate, target_sr)(signal[:1,:])\n",
        "\n",
        "        return resampled, target_sr\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def standardize_duration(aud, max_time):\n",
        "        #standardize all audio files to the same length by either extending duration with silence or truncating it\n",
        "        signal, sample_rate = aud\n",
        "        num_of_rows, signal_length = signal.shape\n",
        "        max_length = sample_rate//1000 * max_time\n",
        "\n",
        "        if (signal_length > max_length):\n",
        "            #truncate signal to given length\n",
        "            signal = signal[:, :max_length]\n",
        "\n",
        "        elif (signal_length < max_length):\n",
        "            #length of padding to add\n",
        "            padding_len = max_length - signal_length\n",
        "\n",
        "            #pad with 0s\n",
        "            padding = torch.zeros(num_of_rows, padding_len)\n",
        "\n",
        "            signal = torch.cat((signal, padding), 1)\n",
        "\n",
        "        return signal, sample_rate\n",
        "\n",
        "    @staticmethod\n",
        "    def time_shift(aud, shift_limit):\n",
        "        #data augmentation on raw audio by time shifting to left/right by a random amount\n",
        "        signal, sample_rate = aud\n",
        "        _, signal_length = signal.shape\n",
        "        amount_to_shift = int(random.random() * shift_limit * signal_length)\n",
        "\n",
        "        return signal.roll(amount_to_shift), sample_rate\n",
        "\n",
        "    @staticmethod\n",
        "    def spectro_gram(aud, n_mels=64, n_fft = 1024, hop_len=None):\n",
        "        #convert augmented audio to a mel spectrogram\n",
        "        signal, sample_rate = aud\n",
        "        top_db = 100\n",
        "\n",
        "        spec = transforms.MelSpectrogram(sample_rate, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(signal)\n",
        "\n",
        "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
        "        return spec\n",
        "\n",
        "    @staticmethod\n",
        "    def mel_spectrogram_augment(spec, max_mask=0.1, n_freq_masks=1, n_time_masks=1):\n",
        "        #another round of augmentation, on mel spectrogram rather than raw data\n",
        "        #frequency mask and time mask\n",
        "        _, n_mels, n_steps = spec.shape\n",
        "        mask_value = spec.mean()\n",
        "        aug_spec = spec\n",
        "\n",
        "        #freq_mask_param: max possible time of the mask\n",
        "        freq_mask_param = max_mask * n_mels\n",
        "        for _ in range (n_freq_masks):\n",
        "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
        "\n",
        "        #time_mask_param: max possible time of the mask\n",
        "        time_mask_param = max_mask * n_steps\n",
        "        for _ in range (n_time_masks):\n",
        "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
        "\n",
        "        return aug_spec\n",
        "\n",
        "def example_spec_from_aud(audio_path):\n",
        "    # Sample preprocessing pipeline\n",
        "    aud = AudioUtil.open(audio_path)\n",
        "    #reaud = AudioUtil.resampling(aud, 8000) #change the number to the sampling rate you want, here is 8khz\n",
        "    mono = AudioUtil.standardize_channel(aud)\n",
        "    fixed_duration = AudioUtil.standardize_duration(mono, 5000) #duration = 5s\n",
        "    shift_aud = AudioUtil.time_shift(fixed_duration, 0.4) #40%\n",
        "    spectrogram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "    #n_mels = 64 because that is the normal speaking vocal range\n",
        "    aug_sgram = AudioUtil.mel_spectrogram_augment(spectrogram, max_mask=0.1, n_freq_masks=2, n_time_masks=2)\n",
        "    return aug_sgram\n",
        "\n",
        "\n",
        "#######################################\n",
        "###     preprocessing functions     ###\n",
        "#######################################\n",
        "\n",
        "def raw_from_aud(aud):\n",
        "    # Takes in raw aud, outputs preprocessed raw signal\n",
        "    # Edit this to tune preprocessing\n",
        "    \n",
        "    new_aud = AudioUtil.resampling(aud, 8000) #change the number to the sampling rate you want, here is 8khz\n",
        "    new_aud = AudioUtil.standardize_channel(new_aud)\n",
        "    signal, sr = AudioUtil.standardize_duration(new_aud, 5000) #duration = 5s\n",
        "\n",
        "    return signal\n",
        "\n",
        "def spec_from_aud(aud):\n",
        "    # Takes in raw aud, outputs preprocessed mel spectrogram\n",
        "    # Edit this to tune preprocessing\n",
        "\n",
        "    new_aud = AudioUtil.standardize_channel(aud)\n",
        "    new_aud = AudioUtil.standardize_duration(new_aud, 5460) #duration = 5.46s to get mel output length of 512\n",
        "    new_aud = AudioUtil.time_shift(new_aud, 0.4) #40%\n",
        "    spectrogram = AudioUtil.spectro_gram(new_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "    #n_mels = 64 because that is the normal speaking vocal range\n",
        "    spectrogram = AudioUtil.mel_spectrogram_augment(spectrogram, max_mask=0.1, n_freq_masks=2, n_time_masks=2)\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "def print_spectrogram(tensor_input):\n",
        "  #time vs amplitude\n",
        "  two_d_spec = tensor_input[0]\n",
        "  return plt.imshow(two_d_spec.permute(0,1))\n",
        "\n",
        "PREPROCESSOR = spec_from_aud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlFu63MhVlfW"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLh3AQVFxoV5",
        "outputId": "71dce174-b46b-4db3-a256-8aaaa1a1b290"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3bf35032f0>"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define training parameters\n",
        "\n",
        "loss_function = F.nll_loss\n",
        "\n",
        "optimizer = torch.optim.Adam\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 50\n",
        "torch.manual_seed(28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1YWxvMXXmpF"
      },
      "source": [
        "We define the model here for the model file to be loaded into a PyTorch model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BRee32EViH6"
      },
      "outputs": [],
      "source": [
        "# CNN model implementation for Mels spectrogram\n",
        "class CustomModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=6, any_other_params_you_need=None):\n",
        "        super().__init__()\n",
        "        self.H, self.W = 64, 512  # Size of mel_spectrogram\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define your layers here:\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=\"same\")\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.maxpool1 = nn.MaxPool2d(2, stride=2) #max pooling\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 16, 5, padding=\"same\")\n",
        "        self.maxpool2 = nn.MaxPool2d(2, stride=1) #max pooling\n",
        "\n",
        "        self.conv3 = nn.Conv2d(16, 8, 2, padding=\"same\")\n",
        "        self.bn2 = nn.BatchNorm2d(8)\n",
        "        self.fc1 = nn.Linear(8 * 31 * 255, self.num_classes)\n",
        "\n",
        "        self.pooling2x2 = lambda x: F.max_pool2d(x, 2, stride=2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = F.relu(self.conv1(inputs))\n",
        "        x = self.bn1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x = x.view(inputs.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TF2lD6kvxSj"
      },
      "outputs": [],
      "source": [
        "class LightningModel(pl.LightningModule):\n",
        "    def __init__(self, model, learning_rate=1e-3, loss_function=F.nll_loss, optimizer=torch.optim.Adam, weight_decay=1e-6):\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_function = loss_function\n",
        "        # Weight decay for L2 regularization\n",
        "        self.optimizer = optimizer(model.parameters(), lr=self.learning_rate, weight_decay=weight_decay)\n",
        "        self.model = model\n",
        "        \n",
        "        self.train_acc = torchmetrics.Accuracy()\n",
        "        self.val_acc = torchmetrics.Accuracy()\n",
        "        self.test_acc = torchmetrics.Accuracy()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # in lightning, forward defines the prediction/inference actions\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # training_step defined the train loop.\n",
        "        # It is independent of forward\n",
        "        x, y = batch\n",
        "        output = self(x)  # Call self.forward function\n",
        "        loss = self.loss_function(output, y)\n",
        "        self.train_acc(output, y)\n",
        "        # Logging to TensorBoard by default\n",
        "        self.log(\"train_loss\", loss, on_epoch=True)\n",
        "        self.log(\"train_acc\", self.train_acc, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # training_step defined the train loop.\n",
        "        # It is independent of forward\n",
        "        x, y = batch\n",
        "        output = self(x)  # Call self.forward function\n",
        "        loss = self.loss_function(output, y)\n",
        "        self.val_acc(output, y)\n",
        "        # Logging to TensorBoard by default\n",
        "        self.log(\"val_loss\", loss, on_epoch=True)\n",
        "        self.log(\"val_acc\", self.val_acc, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # training_step defined the train loop.\n",
        "        # It is independent of forward\n",
        "        x, y = batch\n",
        "        output = self(x)  # Call self.forward function\n",
        "        loss = self.loss_function(output, y)\n",
        "        self.test_acc(output, y)\n",
        "        # Logging to TensorBoard by default\n",
        "        self.log(\"test_loss\", loss, on_epoch=True)\n",
        "        self.log(\"test_acc\", self.test_acc, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return self.optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9m087-HuKiX"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXYMEBwvYEE1"
      },
      "source": [
        "Once the model has been loaded, when we run `print(model)`, we should see a summary of the model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwQKAmIIuPXB",
        "outputId": "3d09b986-c805-4256-c19a-2a0815c71b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightningModel(\n",
            "  (model): CustomModel(\n",
            "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv2): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "    (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3): Conv2d(16, 8, kernel_size=(2, 2), stride=(1, 1), padding=same)\n",
            "    (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (fc1): Linear(in_features=63240, out_features=6, bias=True)\n",
            "  )\n",
            "  (train_acc): Accuracy()\n",
            "  (val_acc): Accuracy()\n",
            "  (test_acc): Accuracy()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Initialise a new model and load the state\n",
        "model = LightningModel(CustomModel(), learning_rate=learning_rate, loss_function=loss_function, optimizer=optimizer)\n",
        "\n",
        "# code for loading checkpoint file\n",
        "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bJTNqqerBLN"
      },
      "source": [
        "## Let the model guess your age!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9skn9HNplLw",
        "outputId": "4e6d9ec8-8bb3-484e-d5bf-125489e369b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: ipywebrtc in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (5.3.1)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchaudio) (4.1.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook) (0.13.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook) (5.3.0)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook) (5.1.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook) (5.6.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook) (5.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook) (2.11.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook) (0.2.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook) (1.8.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook) (4.10.1)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook) (4.9.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook) (22.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client>=5.2.0->notebook) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook) (0.7.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->notebook) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook) (2.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (5.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook) (1.5.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook) (2.15.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook) (5.7.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook) (4.11.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook) (3.8.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook) (0.5.1)\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n"
          ]
        }
      ],
      "source": [
        "# set up microphone audio recorder\n",
        "\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "\n",
        "from ipywebrtc import AudioRecorder, CameraStream\n",
        "import torchaudio\n",
        "from IPython.display import Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfCWwdK5qNWv"
      },
      "outputs": [],
      "source": [
        "# initialise audio recorder\n",
        "\n",
        "camera = CameraStream(constraints={'audio': True,'video':False})\n",
        "recorder = AudioRecorder(stream=camera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcBePRwGYx6o"
      },
      "source": [
        "# **Record your audio here!**\n",
        "\n",
        "Click on the dot button to start recording, and click on it again to stop recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107,
          "referenced_widgets": [
            "58acd8e6f8d5409484b32ced3b49707d",
            "d7f4af699e1a491f9de6ac21bfd1c75b",
            "cfd5520656a04f3c8a2b6dc77de9274a",
            "c3ed5f5b1fce4cf28cacbfa4a7309494",
            "0353b6212c7f4e11b0090c179836d0a2",
            "ed9069eb8905422b85da9248994f5b4c"
          ]
        },
        "id": "TF8A6osuqRoB",
        "outputId": "4578e1c5-3966-4052-8bba-b662a69fa25b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58acd8e6f8d5409484b32ced3b49707d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "AudioRecorder(audio=Audio(value=b'', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': â€¦"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/a8874ba6619b6106/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# runs audio recorder interface\n",
        "\n",
        "recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT7itfSLqvw9"
      },
      "outputs": [],
      "source": [
        "# convert audio recording into .wav, here we play the .wav audio to ensure that it has been converted successfully\n",
        "\n",
        "!wget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz\n",
        "!tar xJf ffmpeg-release-amd64-static.tar.xz\n",
        "\n",
        "with open('recording.webm', 'wb') as f:\n",
        "    f.write(recorder.audio.value)\n",
        "!ffmpeg-5.0.1-amd64-static/ffmpeg -i recording.webm -ac 1 -f wav file.wav -y -hide_banner -loglevel panic\n",
        "sig, sr = torchaudio.load(\"file.wav\") # converted audio is saved as file.wav\n",
        "print(sig.shape)\n",
        "Audio(data=sig, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "406dTYzQrTc8",
        "outputId": "dfd740f7-3a9d-4daf-fb13-5240e41e163d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted tensor([0])\n"
          ]
        }
      ],
      "source": [
        "# pass in file.wav into model for prediction\n",
        "\n",
        "pred = model(torch.unsqueeze(PREPROCESSOR(torchaudio.load(\"file.wav\")),0))\n",
        "pred_label = torch.argmax(pred, dim=1)\n",
        "\n",
        "print(\"predicted\", pred_label)\n",
        "\n",
        "label_to_classification = {0: \"TEENS\", 1: \"TWENTIES\", 2: \"THIRTIES\", \n",
        "                           3: \"FOURTIES\", 4: \"FIFTIES\", 5: \"SIXTIES\"}\n",
        "classification = label_to_classification[pred_label.item()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y19MBiA7rAK9"
      },
      "source": [
        "# The model thinks you are.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2zj6tAdZO-6",
        "outputId": "2120f4cc-e9ef-4410-b250-493656b14779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From your voice... \n",
            "\n",
            "The model thinks you are in your TEENS!\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"From your voice... \n",
        "\n",
        "The model thinks you are in your {classification}!\n",
        "  \"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Project Code for GUI",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0353b6212c7f4e11b0090c179836d0a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58acd8e6f8d5409484b32ced3b49707d": {
          "model_module": "jupyter-webrtc",
          "model_module_version": "~0.6.0",
          "model_name": "AudioRecorderModel",
          "state": {
            "_data_src": "blob:https://lioo294yau8-496ff2e9c6d22116-0-colab.googleusercontent.com/d9c456f1-ce34-4318-8ec9-71d9f0630203",
            "_dom_classes": [],
            "_model_module": "jupyter-webrtc",
            "_model_module_version": "~0.6.0",
            "_model_name": "AudioRecorderModel",
            "_view_count": null,
            "_view_module": "jupyter-webrtc",
            "_view_module_version": "~0.6.0",
            "_view_name": "AudioRecorderView",
            "audio": "IPY_MODEL_d7f4af699e1a491f9de6ac21bfd1c75b",
            "autosave": false,
            "codecs": "",
            "filename": "record",
            "format": "webm",
            "layout": "IPY_MODEL_cfd5520656a04f3c8a2b6dc77de9274a",
            "recording": false,
            "stream": "IPY_MODEL_c3ed5f5b1fce4cf28cacbfa4a7309494"
          }
        },
        "c3ed5f5b1fce4cf28cacbfa4a7309494": {
          "model_module": "jupyter-webrtc",
          "model_module_version": "~0.6.0",
          "model_name": "CameraStreamModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "jupyter-webrtc",
            "_model_module_version": "~0.6.0",
            "_model_name": "CameraStreamModel",
            "_view_count": null,
            "_view_module": "jupyter-webrtc",
            "_view_module_version": "~0.6.0",
            "_view_name": "MediaStreamView",
            "constraints": {
              "audio": true,
              "video": false
            },
            "layout": "IPY_MODEL_ed9069eb8905422b85da9248994f5b4c"
          }
        },
        "cfd5520656a04f3c8a2b6dc77de9274a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f4af699e1a491f9de6ac21bfd1c75b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "AudioModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "AudioModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "AudioView",
            "autoplay": true,
            "controls": true,
            "format": "webm",
            "layout": "IPY_MODEL_0353b6212c7f4e11b0090c179836d0a2",
            "loop": true
          }
        },
        "ed9069eb8905422b85da9248994f5b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
